{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> Setup Hadoop Locally on your Workstation</h1>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Download and Install JDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "``` bash\n",
    "\n",
    "# Mac\n",
    "\n",
    "# If you have brew installed\n",
    "\n",
    "brew tap caskroom/versions\n",
    "brew cask install java8\n",
    "\n",
    "# If you DO NOT have brew installed \n",
    "\n",
    "/usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\"\n",
    "brew tap caskroom/versions\n",
    "brew cask install java8\n",
    "\n",
    "\n",
    "# ELSE (you may want to speak to your instructor if not using Mac)\n",
    "\n",
    "# CentOS\n",
    "# You may need to update the URL per latest versions\n",
    "\n",
    "wget -c --header \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.rpm\n",
    "yum localinstall jdk-8u121-linux-x64.rpm\n",
    "\n",
    "\n",
    "cd ~/Downloads\n",
    "curl -v -j -k -L -H \"Cookie: oraclelicense=accept-securebackup-cookie\" http://download.oracle.com/otn-pub/java/jdk/8u112-b15/jre-8u112-macosx-x64.dmg > jre-8u112-macosx-x64.dmg\n",
    "hdiutil attach jre-8u112-macosx-x64.dmg\n",
    "sudo installer -pkg /Volumes/Java\\ 8\\ Update\\ 112/Java\\ 8\\ Update\\ 112.app/Contents/Resources/JavaAppletPlugin.pkg -target /\n",
    "diskutil umount /Volumes/Java\\ 8\\ Update\\ 112 \n",
    "rm jre-8u112-macosx-x64.dmg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure JDK 1.8 is installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Enable SSH and Keyless Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "``` bash\n",
    "sudo systemsetup -setremotelogin on\n",
    "\n",
    "ssh-keygen -t rsa\n",
    "cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys\n",
    "\n",
    "ssh localhost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Set JAVA_HOME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add following lines in your ***.bash_profile*** or ***.bashrc***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "export JAVA_HOME=$(/usr/libexec/java_home)\n",
    "\n",
    "# For those of you who don't have java_home in your path add it like this.\n",
    "\n",
    "sudo ln -s /System/Library/Frameworks/JavaVM.framework/Versions/Current/Commands/java_home /usr/libexec/java_home\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Install Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```bash\n",
    "brew install hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following commmand\n",
    "\n",
    "```\n",
    "cd /usr/local/opt/hadoop/\n",
    "\n",
    "bin/hadoop\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will display the usage documentation for the hadoop script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Operations\n",
    "```\n",
    "mkdir input\n",
    "cp etc/hadoop/*.xml input\n",
    "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output 'dfs[a-z.]+'\n",
    "cat output/*\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Distributed Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations\n",
    "\n",
    "Next, you have to start configuring a couple of files. Go to /usr/local/opt/hadoop. In there you will need to make some changes or create the following files\n",
    "\n",
    "1. hadoop-env.sh\n",
    "2. core-site.xml\n",
    "3. mapred-site.xml\n",
    "4. hdfs-site.xml\n",
    "\n",
    "In *hadoop-env.sh* look for\n",
    "\n",
    "```\n",
    "export HADOOP_OPTS=\"$HADOOP_OPTS -Djava.net.preferIPv4Stack=true\"\n",
    "```\n",
    "\n",
    "Replace it with\n",
    "\n",
    "```\n",
    "export HADOOP_OPTS=\"$HADOOP_OPTS -Djava.net.preferIPv4Stack=true -Djava.security.krb5.realm= -Djava.security.krb5.kdc=\"\n",
    "export JAVA_HOME=$(/usr/libexec/java_home)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use the following:**\n",
    "\n",
    "cd /usr/local/opt/hadoop\n",
    "\n",
    "Update the following files with the connect as below:\n",
    "\n",
    "**vi libexec/etc/hadoop/core-site.xml**\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "**vi libexec/etc/hadoop/hdfs-site.xml:**\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "verify keyless access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution\n",
    "\n",
    "#### Format the File System:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start NameNode and DataNode daemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browse the web interface for the NameNode; by default it is available at:\n",
    "\n",
    "NameNode - http://localhost:9870/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the HDFS directories required to execute MapReduce jobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/hdfs dfs -mkdir /user\n",
    "bin/hdfs dfs -mkdir /user/<username>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy the input files into the distributed filesystem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/hdfs dfs -mkdir input\n",
    "bin/hdfs dfs -put etc/hadoop/*.xml input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run some of the examples provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar grep input output 'dfs[a-z.]+'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin/hdfs dfs -get output output\n",
    "cat output/*\n",
    "\n",
    "#OR \n",
    "\n",
    "bin/hdfs dfs -cat output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Job did run in local (defualt) mode. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition.\n",
    "\n",
    "```\n",
    "vi libexec/etc/hadoop/mapred-site.xml\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>mapreduce.application.classpath</name>\n",
    "        <value>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vi libexec/etc/hadoop/yarn-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.env-whitelist</name>\n",
    "        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>\n",
    "    </property>\n",
    "</configuration>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbin/start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Browse the web interface for the ResourceManager; by default it is available at:\n",
    "\n",
    "ResourceManager - http://localhost:8088/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can configure hadoop home to be able to stop and start Hadoop services from anywhere otherwise you need to be in /usr/local/opt/hadoop everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
